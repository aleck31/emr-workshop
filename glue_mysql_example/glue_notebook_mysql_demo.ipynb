{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# AWS Glue Notebook 访问 MySQL RDS 数据库示例\n",
				"\n",
				"本示例展示如何在 AWS Glue Notebook 中安全地访问 MySQL RDS 数据库。我们将使用 Glue connection 连接到 VPC 内的 RDS 数据库并获取数据库凭证，使用 Spark JDBC 连接到数据库。"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"tags": []
			},
			"source": [
				"## 1. 配置 Glue 会话"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Installed kernel version: 1.0.8 \n",
						"Connections to be included:\n",
						"rds-mysql-connection\n"
					]
				}
			],
			"source": [
				"# %idle_timeout 60\n",
				"# %glue_version 5.0\n",
				"# %worker_type G.1X\n",
				"# %number_of_workers 2\n",
				"%connections rds-mysql-connection\n",
				"%additional_python_modules mysql-connector-python"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%config"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 2. 初始化 Glue 上下文"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Trying to create a Glue session for the kernel.\n",
						"Session Type: glueetl\n",
						"Session ID: d7b08ede-910a-4e0e-b517-f01f8103041a\n",
						"Applying the following default arguments:\n",
						"--glue_kernel_version 1.0.8\n",
						"--enable-glue-datacatalog true\n",
						"Waiting for session d7b08ede-910a-4e0e-b517-f01f8103041a to get into ready status...\n",
						"Session d7b08ede-910a-4e0e-b517-f01f8103041a has been created.\n",
						"\n"
					]
				}
			],
			"source": [
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"import boto3\n",
				"import json\n",
				"from pyspark.sql import DataFrame\n",
				"import pandas as pd\n",
				"\n",
				"# 初始化 Glue 上下文\n",
				"# Glue notebook 交互式会话中 SparkContext 已自动创建\n",
				"# sc = SparkContext().getOrCreate()\n",
				"# 直接获取当前的 SparkContext\n",
				"sc = SparkContext._active_spark_context\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Stopping session: 244fb699-d20c-4adb-93e7-320c9cc53d4b\n",
						"Stopped session.\n"
					]
				}
			],
			"source": [
				"# 手动结束状态异常的session\n",
				"%stop_session"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 3. 从 Secrets Manager 获取数据库连接信息(可选)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Database Host: test-mysql-public.cprl6bvujco2.ap-southeast-1.rds.amazonaws.com\n",
						"Database Name: testdb\n",
						"Database Port: 3306\n"
					]
				}
			],
			"source": [
				"def get_secret(secret_id, region_name):\n",
				"    \"\"\"从 Secrets Manager 获取数据库凭证\"\"\"\n",
				"    client = boto3.client('secretsmanager', region_name=region_name)\n",
				"    response = client.get_secret_value(SecretId=secret_id)\n",
				"    secret = json.loads(response['SecretString'])\n",
				"    return secret\n",
				"\n",
				"# 获取 MySQL 连接信息\n",
				"secret_id = \"test/mysql-local\"\n",
				"region_name = \"ap-southeast-1\"  # 使用新加坡区域\n",
				"db_credentials = get_secret(secret_id, region_name)\n",
				"\n",
				"# 打印连接信息（不包含敏感信息）\n",
				"print(f\"Database Host: {db_credentials['host']}\")\n",
				"print(f\"Database Name: {db_credentials['dbname']}\")\n",
				"print(f\"Database Port: {db_credentials['port']}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 4. 测试数据库可访问性(可选)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 11,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+---------------+\n",
						"|connection_test|\n",
						"+---------------+\n",
						"|              1|\n",
						"+---------------+\n",
						"\n",
						"✅ 数据库连接测试成功!\n",
						"\n",
						"方法1: 使用 INFORMATION_SCHEMA 查询表:\n",
						"+-----------------+\n",
						"|       TABLE_NAME|\n",
						"+-----------------+\n",
						"|      departments|\n",
						"|employee_projects|\n",
						"|        employees|\n",
						"|         projects|\n",
						"+-----------------+\n",
						"\n",
						"\n",
						"方法2: 使用 mysql.innodb_table_stats 系统表:\n",
						"+-----------------+\n",
						"|       table_name|\n",
						"+-----------------+\n",
						"|      departments|\n",
						"|employee_projects|\n",
						"|        employees|\n",
						"|         projects|\n",
						"+-----------------+\n"
					]
				}
			],
			"source": [
				"# 获取连接配置\n",
				"connection_options = glueContext.extract_jdbc_conf(\"rds-mysql-connection\")\n",
				"\n",
				"def test_database_connection():\n",
				"    \"\"\"测试数据库连接是否可用\"\"\"\n",
				"    try:       \n",
				"        # 创建一个简单的测试查询\n",
				"        test_df = spark.read.format(\"jdbc\") \\\n",
				"            .options(**connection_options) \\\n",
				"            .option(\"query\", \"SELECT 1 as connection_test\") \\\n",
				"            .load()\n",
				"        \n",
				"        # 显示结果\n",
				"        test_df.show()\n",
				"        print(\"✅ 数据库连接测试成功!\")\n",
				"        \n",
				"        # 方法1: 使用 INFORMATION_SCHEMA 查询\n",
				"        print(\"\\n方法1: 使用 INFORMATION_SCHEMA 查询表:\")\n",
				"        try:\n",
				"            tables_df1 = spark.read.format(\"jdbc\") \\\n",
				"                .options(**connection_options) \\\n",
				"                .option(\"query\", \"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'testdb'\") \\\n",
				"                .load()\n",
				"            tables_df1.show()\n",
				"        except Exception as e:\n",
				"            print(f\"方法1失败: {e}\")\n",
				"        \n",
				"        # 方法2: 使用 mysql.innodb_table_stats 系统表\n",
				"        print(\"\\n方法2: 使用 mysql.innodb_table_stats 系统表:\")\n",
				"        try:\n",
				"            tables_df2 = spark.read.format(\"jdbc\") \\\n",
				"                .options(**connection_options) \\\n",
				"                .option(\"query\", \"SELECT table_name FROM mysql.innodb_table_stats WHERE database_name = 'testdb'\") \\\n",
				"                .load()\n",
				"            tables_df2.show()\n",
				"        except Exception as e:\n",
				"            print(f\"方法2失败: {e}\")\n",
				"        \n",
				"        return True\n",
				"    except Exception as e:\n",
				"        print(f\"❌ 数据库连接测试失败: {e}\")\n",
				"        return False\n",
				"\n",
				"# 执行连接测试\n",
				"connection_successful = test_database_connection()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 5. 使用 Spark JDBC 读取数据（推荐方法）"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 21,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+---+---------------+-------------------+----------+\n",
						"| id|           name|           location|    budget|\n",
						"+---+---------------+-------------------+----------+\n",
						"|  1|    Engineering|Building A, Floor 2|1500000.00|\n",
						"|  2|      Marketing|Building B, Floor 1| 800000.00|\n",
						"|  3|        Finance|Building A, Floor 3|1200000.00|\n",
						"|  4|Human Resources|Building B, Floor 2| 500000.00|\n",
						"|  5|          Sales|Building C, Floor 1|2000000.00|\n",
						"+---+---------------+-------------------+----------+\n"
					]
				}
			],
			"source": [
				"# 使用 Spark SQL 读取 MySQL 数据\n",
				"\n",
				"table_name = 'testdb.departments'\n",
				"\n",
				"# 通过 Glue connection 访问数据库时，无需指定连接信息\n",
				"   # .option(\"url\", \"jdbc:mysql://test-mysql-public.cprl6bvujco2.ap-southeast-1.rds.amazonaws.com:3306/testdb\") \\\n",
				"   # .option(\"user\", \"admin\") \\\n",
				"   # .option(\"password\", \"YourPWD\") \\\n",
				"df = spark.read.format(\"jdbc\") \\\n",
				"   .options(**connection_options) \\\n",
				"   .option(\"dbtable\", table_name) \\\n",
				"   .load()\n",
				"\n",
				"# 显示数据\n",
				"df.show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 33,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Reading table testdb.employees using Spark JDBC...\n",
						"员工表结构:\n",
						"root\n",
						" |-- id: integer (nullable = true)\n",
						" |-- first_name: string (nullable = true)\n",
						" |-- last_name: string (nullable = true)\n",
						" |-- email: string (nullable = true)\n",
						" |-- department: string (nullable = true)\n",
						" |-- salary: decimal(10,2) (nullable = true)\n",
						" |-- hire_date: date (nullable = true)\n",
						"\n",
						"\n",
						"员工数据预览:\n",
						"+---+----------+---------+--------------------+---------------+--------+----------+\n",
						"| id|first_name|last_name|               email|     department|  salary| hire_date|\n",
						"+---+----------+---------+--------------------+---------------+--------+----------+\n",
						"|  1|      John|    Smith|john.smith@exampl...|    Engineering|85000.00|2018-06-15|\n",
						"|  2|     Emily|  Johnson|emily.johnson@exa...|      Marketing|72000.00|2019-03-22|\n",
						"|  3|   Michael| Williams|michael.williams@...|        Finance|95000.00|2017-11-08|\n",
						"|  4|     Sarah|    Brown|sarah.brown@examp...|Human Resources|68000.00|2020-01-15|\n",
						"|  5|     David|    Jones|david.jones@examp...|    Engineering|92000.00|2018-09-30|\n",
						"+---+----------+---------+--------------------+---------------+--------+----------+\n",
						"only showing top 5 rows\n"
					]
				}
			],
			"source": [
				"def read_with_spark_jdbc(table_name):\n",
				"    \"\"\"使用 Spark JDBC 读取数据（推荐用于大数据集）\"\"\"\n",
				"    print(f\"Reading table {table_name} using Spark JDBC...\")\n",
				"\n",
				"    # 检查必要的键是否存在\n",
				"    required_keys = [\"url\", \"user\", \"password\"]\n",
				"    for key in required_keys:\n",
				"        if key not in connection_options:\n",
				"            raise KeyError(f\"Required key '{key}' not found in connection_options\")\n",
				"    \n",
				"    # 从 connection_options 中提取连接信息\n",
				"    jdbc_url = connection_options[\"url\"]  # 或者使用 fullUrl: connection_options[\"fullUrl\"]\n",
				"    username = connection_options[\"user\"]\n",
				"    password = connection_options[\"password\"]\n",
				"    \n",
				"    # 构建连接属性\n",
				"    connection_properties = {\n",
				"        \"user\": username,\n",
				"        \"password\": password,\n",
				"        \"driver\": \"com.mysql.jdbc.Driver\"\n",
				"    }\n",
				"    \n",
				"    df = spark.read.jdbc(\n",
				"        url=jdbc_url,\n",
				"        table=table_name,\n",
				"        properties=connection_properties\n",
				"    )\n",
				"    return df\n",
				"\n",
				"# 读取员工表\n",
				"employees_df = read_with_spark_jdbc(\"testdb.employees\")\n",
				"print(\"员工表结构:\")\n",
				"employees_df.printSchema()\n",
				"print(\"\\n员工数据预览:\")\n",
				"employees_df.show(5)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 6. 使用 Glue DynamicFrame 读取数据"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 40,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Reading table testdb.departments using Glue DynamicFrame...\n",
						"Successfully read 5 records from testdb.departments\n",
						"部门表结构:\n",
						"root\n",
						"|-- id: int\n",
						"|-- name: string\n",
						"|-- location: string\n",
						"|-- budget: decimal\n",
						"\n",
						"\n",
						"部门数据预览:\n",
						"{\"id\": 2, \"name\": \"Marketing\", \"location\": \"Building B, Floor 1\", \"budget\": 800000.00}\n",
						"{\"id\": 3, \"name\": \"Finance\", \"location\": \"Building A, Floor 3\", \"budget\": 1200000.00}\n",
						"{\"id\": 4, \"name\": \"Human Resources\", \"location\": \"Building B, Floor 2\", \"budget\": 500000.00}\n",
						"{\"id\": 1, \"name\": \"Engineering\", \"location\": \"Building A, Floor 2\", \"budget\": 1500000.00}\n",
						"{\"id\": 5, \"name\": \"Sales\", \"location\": \"Building C, Floor 1\", \"budget\": 2000000.00}\n"
					]
				}
			],
			"source": [
				"connection_name=\"rds-mysql-connection\"\n",
				"\n",
				"def read_with_glue_dynamicframe(full_table_name):\n",
				"    \"\"\"使用 AWS Glue 动态帧读取数据\"\"\"\n",
				"    print(f\"Reading table {full_table_name} using Glue DynamicFrame...\")\n",
				"\n",
				"    # 从 connection_options 中获取必要的连接信息\n",
				"    jdbc_url = connection_options[\"url\"]\n",
				"    user = connection_options[\"user\"]\n",
				"    password = connection_options[\"password\"]\n",
				"    \n",
				"    # 构建 DynamicFrame 的连接选项\n",
				"    df_connection_options = {\n",
				"    #     \"connectionName\": connection_name,\n",
				"    #     \"dbtable\": full_table_name\n",
				"        \"url\": jdbc_url,\n",
				"        \"user\": user,\n",
				"        \"password\": password,\n",
				"        \"dbtable\": full_table_name\n",
				"    }\n",
				"    \n",
				"    try:\n",
				"        # 使用 Glue 连接创建 DynamicFrame\n",
				"        \n",
				"        dynamic_frame = glueContext.create_dynamic_frame.from_options(\n",
				"            connection_type=\"mysql\",\n",
				"            connection_options=df_connection_options\n",
				"        )\n",
				"        \n",
				"        print(f\"Successfully read {dynamic_frame.count()} records from {full_table_name}\")\n",
				"        return dynamic_frame\n",
				"    except Exception as e:\n",
				"        print(f\"Error reading table {full_table_name} with DynamicFrame: {e}\")\n",
				"        return None\n",
				"\n",
				"# 读取部门表\n",
				"departments_dyf = read_with_glue_dynamicframe(\"testdb.departments\")\n",
				"\n",
				"print(\"部门表结构:\")\n",
				"departments_dyf.printSchema()\n",
				"print(\"\\n部门数据预览:\")\n",
				"departments_dyf.show(5)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 7. 执行 SQL 查询"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 45,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Reading table testdb.employees using Spark JDBC...\n",
						"Reading table testdb.departments using Glue DynamicFrame...\n",
						"Successfully read 5 records from testdb.departments\n",
						"按部门统计的员工数量和平均薪资:\n",
						"+---------------+--------------+-------------+\n",
						"|     department|employee_count|   avg_salary|\n",
						"+---------------+--------------+-------------+\n",
						"|    Engineering|             5| 89800.000000|\n",
						"|        Finance|             3| 92000.000000|\n",
						"|      Marketing|             3| 75000.000000|\n",
						"|          Sales|             2|107500.000000|\n",
						"|Human Resources|             2| 66500.000000|\n",
						"+---------------+--------------+-------------+\n"
					]
				}
			],
			"source": [
				"# 读取员工表\n",
				"employees_df = read_with_spark_jdbc(\"testdb.employees\")\n",
				"\n",
				"# 将数据帧注册为临时视图\n",
				"employees_df.createOrReplaceTempView(\"employees_view\")\n",
				"\n",
				"# 确保 departments_dyf 已经正确读取\n",
				"departments_dyf = read_with_glue_dynamicframe(\"testdb.departments\")\n",
				"departments_df = departments_dyf.toDF()\n",
				"departments_df.createOrReplaceTempView(\"departments_view\")\n",
				"\n",
				"# 执行 SQL 查询 - 按部门统计员工数量和平均薪资\n",
				"query = \"\"\"\n",
				"SELECT \n",
				"    e.department, \n",
				"    COUNT(*) as employee_count, \n",
				"    AVG(e.salary) as avg_salary\n",
				"FROM employees_view e\n",
				"GROUP BY e.department\n",
				"ORDER BY employee_count DESC\n",
				"\"\"\"\n",
				"\n",
				"result = spark.sql(query)\n",
				"print(\"按部门统计的员工数量和平均薪资:\")\n",
				"result.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 8. 执行复杂的联表查询"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 46,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Reading table testdb.projects using Spark JDBC...\n",
						"Reading table testdb.employee_projects using Spark JDBC...\n",
						"项目、预算和参与员工数量:\n",
						"+--------------------+--------------+---------------+--------------+\n",
						"|        project_name|project_budget|department_name|employee_count|\n",
						"+--------------------+--------------+---------------+--------------+\n",
						"|     Cloud Migration|     500000.00|    Engineering|             4|\n",
						"|Mobile App Develo...|     420000.00|    Engineering|             3|\n",
						"|Financial System ...|     350000.00|        Finance|             3|\n",
						"|Sales Analytics P...|     300000.00|          Sales|             2|\n",
						"|    Website Redesign|     250000.00|      Marketing|             3|\n",
						"|     Employee Portal|     180000.00|Human Resources|             2|\n",
						"+--------------------+--------------+---------------+--------------+\n"
					]
				}
			],
			"source": [
				"# 读取项目表和员工项目关联表\n",
				"projects_df = read_with_spark_jdbc(\"testdb.projects\")\n",
				"projects_df.createOrReplaceTempView(\"projects_view\")\n",
				"\n",
				"employee_projects_df = read_with_spark_jdbc(\"testdb.employee_projects\")\n",
				"employee_projects_df.createOrReplaceTempView(\"employee_projects_view\")\n",
				"\n",
				"# 执行复杂的联表查询 - 查找每个项目的参与员工和总预算\n",
				"complex_query = \"\"\"\n",
				"SELECT \n",
				"    p.name as project_name, \n",
				"    p.budget as project_budget,\n",
				"    d.name as department_name,\n",
				"    COUNT(DISTINCT ep.employee_id) as employee_count\n",
				"FROM projects_view p\n",
				"JOIN departments_view d ON p.department_id = d.id\n",
				"JOIN employee_projects_view ep ON p.id = ep.project_id\n",
				"GROUP BY p.name, p.budget, d.name\n",
				"ORDER BY p.budget DESC\n",
				"\"\"\"\n",
				"\n",
				"complex_result = spark.sql(complex_query)\n",
				"print(\"项目、预算和参与员工数量:\")\n",
				"complex_result.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 9. 清空表数据"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 47,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"✅ 表 testdb.test_data 已成功清空\n",
						"True\n"
					]
				}
			],
			"source": [
				"def truncate_table(table_name):\n",
				"    \"\"\"清空指定表的所有数据\"\"\"\n",
				"    try:\n",
				"        # 从 connection_options 中获取连接信息\n",
				"        jdbc_url = connection_options[\"url\"]\n",
				"        user = connection_options[\"user\"]\n",
				"        password = connection_options[\"password\"]\n",
				"        \n",
				"        # 确保 URL 包含数据库名称\n",
				"        database_name = \"testdb\"\n",
				"        if database_name not in jdbc_url:\n",
				"            if jdbc_url.endswith(\"/\"):\n",
				"                jdbc_url += database_name\n",
				"            else:\n",
				"                jdbc_url += \"/\" + database_name\n",
				"        \n",
				"        # 注意：TRUNCATE TABLE 不能通过 JDBC 的 query 选项直接执行\n",
				"        # 需要使用 executeUpdate 方法\n",
				"        \n",
				"        # 创建一个临时表来执行 SQL 命令\n",
				"        temp_df = spark.createDataFrame([(\"dummy\",)], [\"col\"])\n",
				"        \n",
				"        # 使用 Spark SQL 执行 TRUNCATE 语句\n",
				"        # 这里我们使用一个变通方法：先删除所有数据，然后重置自增 ID\n",
				"        delete_sql = f\"DELETE FROM {table_name}\"\n",
				"        \n",
				"        temp_df.write \\\n",
				"            .format(\"jdbc\") \\\n",
				"            .option(\"url\", jdbc_url) \\\n",
				"            .option(\"dbtable\", table_name) \\\n",
				"            .option(\"user\", user) \\\n",
				"            .option(\"password\", password) \\\n",
				"            .option(\"truncate\", \"true\") \\\n",
				"            .mode(\"overwrite\") \\\n",
				"            .save()\n",
				"        \n",
				"        print(f\"✅ 表 {table_name} 已成功清空\")\n",
				"        return True\n",
				"    except Exception as e:\n",
				"        print(f\"❌ 清空表 {table_name} 失败: {e}\")\n",
				"        return False\n",
				"\n",
				"# 注意：此步不可逆\n",
				"truncate_table(\"testdb.test_data\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 10. 将结果写回到 MySQL 数据库"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 48,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"要写入的测试数据:\n",
						"+---+---------+---------+--------+\n",
						"| id|     name| location|  budget|\n",
						"+---+---------+---------+--------+\n",
						"|  1|测试部门1|测试地点1|100000.0|\n",
						"|  2|测试部门2|测试地点2|200000.0|\n",
						"|  3|测试部门3|测试地点3|300000.0|\n",
						"+---+---------+---------+--------+\n"
					]
				}
			],
			"source": [
				"def write_to_mysql(dataframe, target_table, write_mode=\"overwrite\"):\n",
				"    \"\"\"将数据写入 MySQL 表\"\"\"\n",
				"    # 从 connection_options 中获取连接信息\n",
				"    jdbc_url = connection_options[\"url\"]\n",
				"    user = connection_options[\"user\"]\n",
				"    password = connection_options[\"password\"]\n",
				"    \n",
				"    # 确保 URL 包含数据库名称\n",
				"    database_name = \"testdb\"\n",
				"    if database_name not in jdbc_url:\n",
				"        if jdbc_url.endswith(\"/\"):\n",
				"            jdbc_url += database_name\n",
				"        else:\n",
				"            jdbc_url += \"/\" + database_name\n",
				"    \n",
				"    # 构建连接属性\n",
				"    connection_properties = {\n",
				"        \"user\": user,\n",
				"        \"password\": password,\n",
				"        \"driver\": \"com.mysql.jdbc.Driver\"\n",
				"    }\n",
				"    \n",
				"    # 确保表名包含数据库名称\n",
				"    if \".\" not in target_table:\n",
				"        full_table_name = f\"{database_name}.{target_table}\"\n",
				"    else:\n",
				"        full_table_name = target_table\n",
				"    \n",
				"    print(f\"Writing data to table {full_table_name}...\")\n",
				"    \n",
				"    try:\n",
				"        dataframe.write.jdbc(\n",
				"            url=jdbc_url,\n",
				"            table=full_table_name,\n",
				"            mode=write_mode,\n",
				"            properties=connection_properties\n",
				"        )\n",
				"        print(f\"✅ 数据已成功写入表 {full_table_name}\")\n",
				"    except Exception as e:\n",
				"        print(f\"❌ 写入数据失败: {e}\")\n",
				"\n",
				"# 创建一个新的数据帧用于写入测试\n",
				"test_data = [\n",
				"    (1, \"测试部门1\", \"测试地点1\", 100000.00),\n",
				"    (2, \"测试部门2\", \"测试地点2\", 200000.00),\n",
				"    (3, \"测试部门3\", \"测试地点3\", 300000.00)\n",
				"]\n",
				"\n",
				"# 创建 DataFrame\n",
				"schema = [\"id\", \"name\", \"location\", \"budget\"]\n",
				"test_df = spark.createDataFrame(test_data, schema)\n",
				"\n",
				"# 显示要写入的数据\n",
				"print(\"要写入的测试数据:\")\n",
				"test_df.show()\n",
				"\n",
				"# 将数据写入新表\n",
				"# write_to_mysql(test_df, \"test_departments\", \"overwrite\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 11. 使用 pandas 进行数据分析（适用于小数据集）"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 49,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"使用 pandas 计算部门薪资统计:\n",
						"        department      mean        min        max\n",
						"0      Engineering   89800.0   85000.00   95000.00\n",
						"1          Finance   92000.0   88000.00   95000.00\n",
						"2  Human Resources   66500.0   65000.00   68000.00\n",
						"3        Marketing   75000.0   72000.00   78000.00\n",
						"4            Sales  107500.0  105000.00  110000.00\n"
					]
				}
			],
			"source": [
				"# 将 Spark DataFrame 转换为 pandas DataFrame 进行更多分析\n",
				"employees_pd = employees_df.toPandas()\n",
				"\n",
				"# 使用 pandas 进行数据分析\n",
				"print(\"使用 pandas 计算部门薪资统计:\")\n",
				"salary_stats = employees_pd.groupby('department')['salary'].agg(['mean', 'min', 'max']).reset_index()\n",
				"print(salary_stats)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 12. 清理资源"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# 删除所有临时视图\n",
				"spark.catalog.clearCache()  # 清除缓存\n",
				"temp_views = spark.catalog.listTables()\n",
				"for view in temp_views:\n",
				"    if view.isTemporary:\n",
				"        spark.catalog.dropTempView(view.name)\n",
				"        print(f\"删除临时视图: {view.name}\")\n",
				"                \n",
				"# 停止 Spark 会话\n",
				"# spark.stop() # 在 Glue Notebook 中不要停止 Spark 会话，因为它由环境管理\n",
				"\n",
				"print(\"资源清理完成!\")"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
